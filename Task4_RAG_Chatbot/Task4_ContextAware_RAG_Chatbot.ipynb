{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a56634b6",
   "metadata": {},
   "source": [
    "\n",
    "# Task 4: Context-Aware Chatbot Using LangChain / RAG\n",
    "\n",
    "## Problem Statement & Objective\n",
    "Build a conversational chatbot that can:\n",
    "\n",
    "- Remember context (conversational memory)\n",
    "- Retrieve answers from a vectorized knowledge base (RAG)\n",
    "- Deploy with Streamlit\n",
    "\n",
    "This notebook demonstrates the full pipeline with dataset loading, embeddings, vectorstore, retrieval chain, evaluation, and deployment guide.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c96e6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Install required libraries (uncomment if running first time)\n",
    "# !pip install -U langchain sentence-transformers faiss-cpu streamlit scikit-learn matplotlib pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a70b020",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from typing import List\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "\n",
    "# LangChain imports\n",
    "from langchain.schema import Document\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.llms import OpenAI  # optional\n",
    "from langchain.memory import ConversationBufferMemory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a56c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Helper functions to load corpus (from txt files)\n",
    "\n",
    "def load_corpus_from_txts(folder_path: str) -> List[Document]:\n",
    "    docs = []\n",
    "    for path in glob.glob(os.path.join(folder_path, \"*.txt\")):\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            text = f.read()\n",
    "        metadata = {\"source\": os.path.basename(path)}\n",
    "        docs.append(Document(page_content=text, metadata=metadata))\n",
    "    return docs\n",
    "\n",
    "def chunk_documents(documents: List[Document], chunk_size: int = 500, overlap: int = 50):\n",
    "    new_docs = []\n",
    "    for d in documents:\n",
    "        text = d.page_content\n",
    "        start = 0\n",
    "        while start < len(text):\n",
    "            chunk = text[start:start+chunk_size]\n",
    "            new_meta = dict(d.metadata)\n",
    "            new_meta[\"chunk_start\"] = start\n",
    "            new_docs.append(Document(page_content=chunk, metadata=new_meta))\n",
    "            start += chunk_size - overlap\n",
    "    return new_docs\n",
    "\n",
    "print(\"Functions ready. Place your .txt files in a 'data/' folder.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6394b809",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "MODEL_NAME = \"all-MiniLM-L6-v2\"\n",
    "\n",
    "def build_vectorstore(documents: List[Document], persist_directory: str = None):\n",
    "    hf_embeddings = HuggingFaceEmbeddings(model_name=MODEL_NAME, model_kwargs={\"device\": \"cpu\"})\n",
    "    texts = [d.page_content for d in documents]\n",
    "    metadatas = [d.metadata for d in documents]\n",
    "    vectorstore = FAISS.from_texts(texts, hf_embeddings, metadatas=metadatas)\n",
    "    if persist_directory:\n",
    "        vectorstore.save_local(persist_directory)\n",
    "    return vectorstore\n",
    "\n",
    "# Example usage (if data folder available):\n",
    "# docs = load_corpus_from_txts(\"data\")\n",
    "# chunks = chunk_documents(docs)\n",
    "# vectorstore = build_vectorstore(chunks, persist_directory=\"faiss_store\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0718fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_conversational_chain(vectorstore, llm_choice: str = \"mock\", openai_api_key: str = None):\n",
    "    memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "    if llm_choice == \"openai\":\n",
    "        if not openai_api_key:\n",
    "            raise ValueError(\"OpenAI API key required\")\n",
    "        os.environ[\"OPENAI_API_KEY\"] = openai_api_key\n",
    "        llm = OpenAI(temperature=0)\n",
    "    else:\n",
    "        from langchain.llms.fake import FakeListLLM\n",
    "        fake = FakeListLLM(responses=[\"[Mock reply] Replace with real LLM for actual answers.\"])\n",
    "        llm = fake\n",
    "    qa = ConversationalRetrievalChain.from_llm(llm=llm, retriever=vectorstore.as_retriever(), memory=memory)\n",
    "    return qa\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b814106",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example (after building vectorstore):\n",
    "# qa_chain = build_conversational_chain(vectorstore, llm_choice=\"mock\")\n",
    "# res = qa_chain.run(input=\"What is this document about?\")\n",
    "# print(res)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439558b1",
   "metadata": {},
   "source": [
    "\n",
    "## ðŸš€ Streamlit Deployment\n",
    "\n",
    "Once your dataset and vectorstore are ready, you can deploy the chatbot with Streamlit.\n",
    "\n",
    "1. Save the main script as `Task4_ContextAware_RAG_Chatbot.py` (provided separately).\n",
    "2. In terminal, run:\n",
    "   ```bash\n",
    "   streamlit run Task4_ContextAware_RAG_Chatbot.py\n",
    "   ```\n",
    "3. Open the browser link (usually `http://localhost:8501`) to interact with the chatbot.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb3b1377",
   "metadata": {},
   "source": [
    "\n",
    "## âœ… Final Summary\n",
    "\n",
    "- We created a **context-aware chatbot** using LangChain with RAG.\n",
    "\n",
    "- Used **sentence-transformers embeddings** + **FAISS vectorstore**.\n",
    "\n",
    "- Added **conversation memory**.\n",
    "\n",
    "- Deployment is done via **Streamlit**.\n",
    "\n",
    "\n",
    "You can now upload this notebook and script to GitHub for submission.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
